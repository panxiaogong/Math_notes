## 条件概率，全概率公式，贝叶斯公式

&emsp;&emsp;我们之前对于概率的讨论总是在一组固定的条件下进行的，也就是除了给定的概率空间之外并无其他的信息。但有些时候会出现一些额外的**附加前提**，比如说抛掷一枚均匀的骰子，已知抛掷出来的点数为奇数，求点数为1的概率。那么这个概率显然就不会是六分之一，而应该是三分之一，计算出这样的结果是因为事件 $B$ “抛掷出来的点数为奇数”对事件 $A$ “点数为1”造成了影响，而我们计算出的点数为1的概率，其实应该是“在抛掷出来的点数为奇数的前提下，点数为1的概率”，我们将其形象地称为**条件概率**，记为 $P(A|B)$ 。

&emsp;&emsp;这种带有条件的概率是相当重要的，在给出具体的定义之前，我们先来看一些比较特殊的情形。在古典概型中，我们要计算 $P(A|B)$ ，其实就是计算事件 $A$ 与 $B$ 共同拥有的样本点占事件 $B$ 中的样本点的个数之比，这就是“在事件 $B$ 的前提下，事件 $A$ 发生的概率”，因为事件 $B$ 已然发生，而事件 $A$ 的发生则意味着两件事情都发生了，对于前面提到过的抛骰子问题，事件 $B$ 所含的样本点个数 $m_B$ 为3，事件 $A\cap B$ 所含的样本点个数 $m_{AB}$ 为1，因此

$$\begin{align\*}
P(A|B)=\frac{m_{AB}}{m_B}=\frac{\frac{m_{AB}}{n}}{\frac{m_{B}}{n}}=\frac{P(AB)}{P(B)}
\end{align\*}$$

倘若是几何概型，若以 $m(A),m(B),m(AB),m(\Omega)$ 分别记事件 $A,B,AB,\Omega$ 所对应的点集的测度，且 $m(B)>0$ ，则

$$\begin{align\*}
P(A|B)=\frac{m(AB)}{m(B)}=\frac{\frac{m(AB)}{m(\Omega)}}{\frac{m(B)}{m(\Omega)}}=\frac{P(AB)}{P(B)}
\end{align\*}$$

在一般场合，我们也将这个式子作为条件概率的定义。

&emsp;&emsp;定义：设($\Omega,\mathscr{F},P$)是一个概率空间， $B\in \mathscr{F}$ 并且 $P(B)>0$ ，则对任意的 $A\in \mathscr{F}$ ，记

$$\begin{align\*}
P(A|B)=\frac{P(AB)}{P(B)}
\end{align\*}$$

并称 $P(A|B)$ 为**在事件** $B$ **发生的条件下事件** $A$ **发生的条件概率（conditional probability）**

> 若未经特别指出，今后出现条件概率 $P(A|B)$ 时，都假定 $P(B)>0$ ，但倘若 $P(B)=0$ ，由于这时 $P(AB)$ 也必须为0，所以计算式为待定型，进一步研究依然是可能的。

&emsp;&emsp;并且可以由条件概率的计算公式得到

$$\begin{align\*}
P(AB)=P(B)P(A|B)
\end{align\*}$$

该等式也被称为概率的**乘法公式**或**乘法定理**，倘若还有 $P(A)>0$ ，则也可以定义出 $P(B|A)$ ，那么

$$\begin{align\*}
P(AB)=P(A)P(B|A)=P(B)P(A|B)
\end{align\*}$$

&emsp;&emsp;下面我们来讨论条件概率的性质。条件概率具有概率的三个基本性质，非负性，规范性，可列可加性。
- $P(A|B)>0$
- $P(\Omega |B)=1$
- $P(\sum_{i=1}^{\infty}A_i|B)=\sum_{i=1}^{\infty}P(A_i|B)$

&emsp;&emsp;因此，类似于概率，对于条件概率也可由三个基本性质导出其他的性质，例如：
- $P(\phi |B)=0$
- $P(A|B)=1-P(\overline{A}|B)$
- $P(A_1\cup A_2|B)=P(A_1|B)+P(A_2|B)-P(A_1A_2|B)$

&emsp;&emsp;另外，我们还可以把乘法公式推广到任意 $n$ 个事件的场合：

$$\begin{align\*}
P(A_1A_2...A_n)=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)...P(A_n|A_1A_2...A_{n-1})
\end{align\*}$$

&emsp;&emsp;下面我们来看一道条件概率的经典例题，波利亚坛子模型。

&emsp;&emsp;设坛子中有 $a$ 个白球及 $b$ 个黑球，每次随机取出一个球，取出后将原球放回，并加入 $c$ 个同色球和 $d$ 个异色球，再摸第二次，这样下去共摸了 $n$ 次，记 $A_i$ 表示第 $i$ 次取出的是白球，记 $B_j$ 表示第 $j$ 次取出的是黑球，试求 $P(A_1A_2...A_{n_1}B_{n_1+1}B_{n_1+2}...B_{n})$ 。

&emsp;&emsp;解：使用乘法公式：

$$\begin{align\*}
P(A_1A_2...A_{n_1}B_{n_1+1}B_{n_1+2}...B_{n})\\
=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)...P(A_{n_1}|A_1A_2...A_{n_1-1})\\
P(B_{n_1+1}|A_1A_2...A_{n_1})P(B_{n_1+2}|A_1A_2...A_{n_1}B_{n_1+1})...P(B_{n}|A_1A_2...A_{n_1}B_{n_1+1}B_{n_1+2}...B_{n-1})\\
=\frac{a}{a+b}\frac{a+c}{a+b+c+d}...\frac{a+(n_1-1)c}{a+b+(n_1-1)(c+d)}\frac{b+n_1d}{a+b+n_1(c+d)}\frac{b+n_1d+c}{a+b+(n_1+1)(c+d)}...\frac{b+n_1d+(n-n_1-1)c}{a+b+(n-1)(c+d)}\\
=\prod_{k=0}^{n_1-1}\frac{a+kc}{a+b+k(c+d)}\prod_{k=n_1}^{n-1}\frac{b+n_1d+(k-n_1)c}{a+b+k(c+d)}
\end{align\*}$$

&emsp;&emsp;这种模型有很多种变化：
- 当 $c=-1,d=0$ 时，即为**不返回抽样**
- 当 $c=0,d=0$ 时，即为**返回抽样**
- 当 $c>0,d=0$ 时，称为**传染病模型**，即每发现一个传染病患者（某种颜色的球），都会增加后续发现其他传染病患者（同色球）的可能性。
- 当 $c=0,d>0$ 时，称为**安全模型**，即每当有事故（比如白球）发生，就会抓紧安全工作（放入黑球），从而降低下一次事故发生的概率。

&emsp;&emsp;在我们知晓了条件概率之后，再结合我们之前提到过的内容，我们能够验证下面的关系式：

$$\begin{align\*}
P(B)=P(AB)+P(\overline{A}B)=P(A)P(B|A)+P(\overline{A})P(B|\overline{A})
\end{align\*}$$

&emsp;&emsp;为什么会有这样的等式？原因很简单，当我们在处理一些比较复杂的事件时，我们需要将其拆分成若干个不相容的简单事件之和，再通过相关性质来进行计算。比如从装有 $a$ 只白球和 $b$ 只黑球的袋子里不放回地摸球，求出第二次摸得黑球的概率。我们倘若设事件 $A$ 为第一次摸得黑球，设事件 $B$ 为第二次摸得黑球，那么根据上面的公式，就有

$$\begin{align\*}
P(B)=P(AB)+P(\overline{A}B)=P(A)P(B|A)+P(\overline{A})P(B|\overline{A})\\
=\frac{b}{a+b}\frac{b-1}{a+b-1}+\frac{a}{a+b}\frac{b}{a+b-1}=\frac{b}{a+b}\\
\end{align\*}$$

那这其实就是摸球与顺序无关，但是这个式子为我们带来了新的理解：后摸者可能会处于不利境况，就是先摸者摸到了黑球，此时后摸者摸到黑球的概率是 $\frac{b-1}{a+b-1}$ ，但是后摸者也可能处于有利境况，也就是先摸者摸到的是白球，此时后摸者摸到黑球的概率是 $\frac{b}{a+b-1}$ ，而综合两种情况考量，最后的结果其实是二者的加权平均，权重分别是 $\frac{b}{a+b}$ 与 $\frac{a}{a+b}$ 。但是在实际的情况中，我们很少会遇到把复杂事件拆分成两个简单事件的情况，往往会拆分成很多个，我们下面来讨论一般的情形。

&emsp;&emsp;若事件 $A_1,A_2,...,A_n,...$ 两两互不相容并且满足：

$$\begin{align\*}
\sum_{i=1}^{\infty}A_i=\Omega
\end{align\*}$$

我们就称事件 $A_1,A_2,...,A_n,...$ 是样本空间的一个**分割**，也叫做**完备事件组**。这样一来，对于任意的事件 $B$ ，就有：

$$\begin{align\*}
B=\sum_{i=1}^{\infty}A_iB
\end{align\*}$$

那么推导就会有：

$$\begin{align\*}
P(B)=\sum_{i=1}^{\infty}P(A_iB)=\sum_{i=1}^{\infty}P(A_i)P(B|A_i)
\end{align\*}$$

该公式被称为**全概率公式**。是概率论中相当常用的一个公式，原因也十分简单：将复杂事件分解然后各个击破比直接考虑复杂事件要简单地多。

&emsp;&emsp;另外，由于

$$\begin{align\*}
P(A_iB)=P(A_i)P(B|A_i)=P(B)P(A_i|B)
\end{align\*}$$

那我们就能得到：

$$\begin{align\*}
P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(B)}
\end{align\*}$$

之后再由全概率公式，就可以获得：

$$\begin{align\*}
P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(B)}=\frac{P(A_i)P(B|A_i)}{\sum_{j=1}^{\infty}P(A_j)P(B|A_j)}
\end{align\*}$$

该公式称为**贝叶斯公式**。其中 $P(A_i)$ 称之为**先验概率**， $P(A_i|B)$ 称之为**后验概率**，而且根据公式不难看出，后验概率的大小很受先验概率大小的影响，而实际上在贝叶斯公式的使用中，最有争议的点就是先验概率的选取。

&emsp;&emsp;如何理解这个公式的意义呢？拿医疗诊断来距离，医生为了诊断病人到底是患了疾病 $A_1,A_2,...,A_n$ 中的哪一种，想通过检查某一个指标 $B$ 来判断（比如体温，脉搏等等），换句话说，就是在指标 $B$ 的前提下，判断出哪一种疾病的概率最高，也就是求出 $P(A_i|B)$ ，那我们就先需要先验概率 $P(A_i)$ ，之后再根据往日的资料等等确定 $P(B|A_i)$ ，最后根据贝叶斯公式求出 $P(A_i|B)$ ，我们需要找到最大的 $P(A_i|B)$ ，对应的 $A_i$ 就是出现了指标 $B$ 的病人最有可能患有的疾病。

> 实际上，在医疗检查中，检查的指标往往会有多个，最后综合所有的后验概率来进行判断，颇有实用价值。

## 事件独立性

&emsp;&emsp;有的时候两个事件是不会相互影响，比如说一口袋里有 $a$ 个白球和 $b$ 个黑球，有放回的摸球，那么事件 $B$ 第二次摸到黑球的概率就是 $\frac{a}{a+b}$ ，而且因为是有放回，所以第一次摸到什么颜色的球并不重要，倘若记事件 $A$ 为第一次摸到黑球，那我们不难验证 $P(AB)$ 与 $P(A)P(B)$ 是大小相同的，这很自然，因为第一次摸球根本不会影响到第二次，换句话说，事件 $A$ 与 $B$ 之间有某种“独立性”。

&emsp;&emsp;对此，我们引进该定义：对事件 $A$ 与 $B$ ，如果有：

$$\begin{align\*}
P(AB)=P(A)P(B)
\end{align\*}$$

那么称事件 $A$ 与 $B$ 是**统计独立的**，简称独立的（independent）。按照这个定义，必然事件 $\Omega$ 和不可能事件 $\phi$ 与任何事件独立，而且不难验证，我们有下面的推论：
- 若事件 $A$ 与 $B$ 独立，且 $P(B)>0$ ，则 $P(A|B)=P(A)$
- 若事件 $A$ 与 $B$ 独立，则事件 $\overline{A}$ 与 $B$ 独立， $A$ 与 $\overline{B}$ 独立， $\overline{A}$ 与 $\overline{B}$ 独立（依据定义即可证明）。

&emsp;&emsp;那我们如何定义多个事件的独立性呢？倘若我们有三个事件 $A,B,C$ ，那么我们需要同时满足下面的四个等式：

$$\begin{align\*}
P(AB)=P(A)P(B)\\
P(AC)=P(A)P(C)\\
P(BC)=P(B)P(C)\\
P(ABC)=P(A)P(B)P(C)\\
\end{align\*}$$

我们知道如果有前三个等式成立，就有了事件 $A,B,C$ 两两独立，那为什么还需要第四个等式呢？也就是两两独立是否等于相互独立？我们来看下面的例子：

&emsp;&emsp;


























