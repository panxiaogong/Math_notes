## 条件概率，全概率公式，贝叶斯公式

&emsp;&emsp;我们之前对于概率的讨论总是在一组固定的条件下进行的，也就是除了给定的概率空间之外并无其他的信息。但有些时候会出现一些额外的**附加前提**，比如说抛掷一枚均匀的骰子，已知抛掷出来的点数为奇数，求点数为1的概率。那么这个概率显然就不会是六分之一，而应该是三分之一，计算出这样的结果是因为事件 $B$ “抛掷出来的点数为奇数”对事件 $A$ “点数为1”造成了影响，而我们计算出的点数为1的概率，其实应该是“在抛掷出来的点数为奇数的前提下，点数为1的概率”，我们将其形象地称为**条件概率**，记为 $P(A|B)$ 。

&emsp;&emsp;这种带有条件的概率是相当重要的，在给出具体的定义之前，我们先来看一些比较特殊的情形。在古典概型中，我们要计算 $P(A|B)$ ，其实就是计算事件 $A$ 与 $B$ 共同拥有的样本点占事件 $B$ 中的样本点的个数之比，这就是“在事件 $B$ 的前提下，事件 $A$ 发生的概率”，因为事件 $B$ 已然发生，而事件 $A$ 的发生则意味着两件事情都发生了，对于前面提到过的抛骰子问题，事件 $B$ 所含的样本点个数 $m_B$ 为3，事件 $A\cap B$ 所含的样本点个数 $m_{AB}$ 为1，因此

$$\begin{align\*}
P(A|B)=\frac{m_{AB}}{m_B}=\frac{\frac{m_{AB}}{n}}{\frac{m_{B}}{n}}=\frac{P(AB)}{P(B)}
\end{align\*}$$

倘若是几何概型，若以 $m(A),m(B),m(AB),m(\Omega)$ 分别记事件 $A,B,AB,\Omega$ 所对应的点集的测度，且 $m(B)>0$ ，则

$$\begin{align\*}
P(A|B)=\frac{m(AB)}{m(B)}=\frac{\frac{m(AB)}{m(\Omega)}}{\frac{m(B)}{m(\Omega)}}=\frac{P(AB)}{P(B)}
\end{align\*}$$

在一般场合，我们也将这个式子作为条件概率的定义。

&emsp;&emsp;定义：设($\Omega,\mathscr{F},P$)是一个概率空间， $B\in \mathscr{F}$ 并且 $P(B)>0$ ，则对任意的 $A\in \mathscr{F}$ ，记

$$\begin{align\*}
P(A|B)=\frac{P(AB)}{P(B)}
\end{align\*}$$

并称 $P(A|B)$ 为**在事件** $B$ **发生的条件下事件** $A$ **发生的条件概率（conditional probability）**

> 若未经特别指出，今后出现条件概率 $P(A|B)$ 时，都假定 $P(B)>0$ ，但倘若 $P(B)=0$ ，由于这时 $P(AB)$ 也必须为0，所以计算式为待定型，进一步研究依然是可能的。

&emsp;&emsp;并且可以由条件概率的计算公式得到

$$\begin{align\*}
P(AB)=P(B)P(A|B)
\end{align\*}$$

该等式也被称为概率的**乘法公式**或**乘法定理**，倘若还有 $P(A)>0$ ，则也可以定义出 $P(B|A)$ ，那么

$$\begin{align\*}
P(AB)=P(A)P(B|A)=P(B)P(A|B)
\end{align\*}$$

&emsp;&emsp;下面我们来讨论条件概率的性质。条件概率具有概率的三个基本性质，非负性，规范性，可列可加性。
- $P(A|B)>0$
- $P(\Omega |B)=1$
- $P(\sum_{i=1}^{\infty}A_i|B)=\sum_{i=1}^{\infty}P(A_i|B)$

&emsp;&emsp;因此，类似于概率，对于条件概率也可由三个基本性质导出其他的性质，例如：
- $P(\phi |B)=0$
- $P(A|B)=1-P(\overline{A}|B)$
- $P(A_1\cup A_2|B)=P(A_1|B)+P(A_2|B)-P(A_1A_2|B)$

&emsp;&emsp;另外，我们还可以把乘法公式推广到任意 $n$ 个事件的场合：

$$\begin{align\*}
P(A_1A_2...A_n)=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)...P(A_n|A_1A_2...A_{n-1})
\end{align\*}$$

&emsp;&emsp;下面我们来看一道条件概率的经典例题，波利亚坛子模型。

&emsp;&emsp;设坛子中有 $a$ 个白球及 $b$ 个黑球，每次随机取出一个球，取出后将原球放回，并加入 $c$ 个同色球和 $d$ 个异色球，再摸第二次，这样下去共摸了 $n$ 次，记 $A_i$ 表示第 $i$ 次取出的是白球，记 $B_j$ 表示第 $j$ 次取出的是黑球，试求 $P(A_1A_2...A_{n_1}B_{n_1+1}B_{n_1+2}...B_{n})$ 。

&emsp;&emsp;解：使用乘法公式：

$$\begin{align\*}
P(A_1A_2...A_{n_1}B_{n_1+1}B_{n_1+2}...B_{n})\\
=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)...P(A_{n_1}|A_1A_2...A_{n_1-1})\\
P(B_{n_1+1}|A_1A_2...A_{n_1})P(B_{n_1+2}|A_1A_2...A_{n_1}B_{n_1+1})...P(B_{n}|A_1A_2...A_{n_1}B_{n_1+1}B_{n_1+2}...B_{n-1})\\
=\frac{a}{a+b}\frac{a+c}{a+b+c+d}...\frac{a+(n_1-1)c}{a+b+(n_1-1)(c+d)}\frac{b+n_1d}{a+b+n_1(c+d)}\frac{b+n_1d+c}{a+b+(n_1+1)(c+d)}...\frac{b+n_1d+(n-n_1-1)c}{a+b+(n-1)(c+d)}\\
=\prod_{k=0}^{n_1-1}\frac{a+kc}{a+b+k(c+d)}\prod_{k=n_1}^{n-1}\frac{b+n_1d+(k-n_1)c}{a+b+k(c+d)}
\end{align\*}$$

&emsp;&emsp;这种模型有很多种变化：
- 当 $c=-1,d=0$ 时，即为**不返回抽样**
- 当 $c=0,d=0$ 时，即为**返回抽样**
- 当 $c>0,d=0$ 时，称为**传染病模型**，即每发现一个传染病患者（某种颜色的球），都会增加后续发现其他传染病患者（同色球）的可能性。
- 当 $c=0,d>0$ 时，称为**安全模型**，即每当有事故（比如白球）发生，就会抓紧安全工作（放入黑球），从而降低下一次事故发生的概率。

&emsp;&emsp;在我们知晓了条件概率之后，再结合我们之前提到过的内容，我们能够验证下面的关系式：

$$\begin{align\*}
P(B)=P(AB)+P(\overline{A}B)=P(A)P(B|A)+P(\overline{A})P(B|\overline{A})
\end{align\*}$$

&emsp;&emsp;为什么会有这样的等式？原因很简单，当我们在处理一些比较复杂的事件时，我们需要将其拆分成若干个不相容的简单事件之和，再通过相关性质来进行计算。比如从装有 $a$ 只白球和 $b$ 只黑球的袋子里不放回地摸球，求出第二次摸得黑球的概率。我们倘若设事件 $A$ 为第一次摸得黑球，设事件 $B$ 为第二次摸得黑球，那么根据上面的公式，就有

$$\begin{align\*}
P(B)=P(AB)+P(\overline{A}B)=P(A)P(B|A)+P(\overline{A})P(B|\overline{A})\\
=\frac{b}{a+b}\frac{b-1}{a+b-1}+\frac{a}{a+b}\frac{b}{a+b-1}=\frac{b}{a+b}\\
\end{align\*}$$

那这其实就是摸球与顺序无关，但是这个式子为我们带来了新的理解：后摸者可能会处于不利境况，就是先摸者摸到了黑球，此时后摸者摸到黑球的概率是 $\frac{b-1}{a+b-1}$ ，但是后摸者也可能处于有利境况，也就是先摸者摸到的是白球，此时后摸者摸到黑球的概率是 $\frac{b}{a+b-1}$ ，而综合两种情况考量，最后的结果其实是二者的加权平均，权重分别是 $\frac{b}{a+b}$ 与 $\frac{a}{a+b}$ 。但是在实际的情况中，我们很少会遇到把复杂事件拆分成两个简单事件的情况，往往会拆分成很多个，我们下面来讨论一般的情形。

&emsp;&emsp;若事件 $A_1,A_2,...,A_n,...$ 两两互不相容并且满足：

$$\begin{align\*}
\sum_{i=1}^{\infty}A_i=\Omega
\end{align\*}$$

我们就称事件 $A_1,A_2,...,A_n,...$ 是样本空间的一个**分割**，也叫做**完备事件组**。这样一来，对于任意的事件 $B$ ，就有：

$$\begin{align\*}
B=\sum_{i=1}^{\infty}A_iB
\end{align\*}$$

那么推导就会有：

$$\begin{align\*}
P(B)=\sum_{i=1}^{\infty}P(A_iB)=\sum_{i=1}^{\infty}P(A_i)P(B|A_i)
\end{align\*}$$

该公式被称为**全概率公式**。是概率论中相当常用的一个公式，原因也十分简单：将复杂事件分解然后各个击破比直接考虑复杂事件要简单地多。

&emsp;&emsp;另外，由于

$$\begin{align\*}
P(A_iB)=P(A_i)P(B|A_i)=P(B)P(A_i|B)
\end{align\*}$$

那我们就能得到：

$$\begin{align\*}
P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(B)}
\end{align\*}$$

之后再由全概率公式，就可以获得：

$$\begin{align\*}
P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(B)}=\frac{P(A_i)P(B|A_i)}{\sum_{j=1}^{\infty}P(A_j)P(B|A_j)}
\end{align\*}$$

该公式称为**贝叶斯公式**。其中 $P(A_i)$ 称之为**先验概率**， $P(A_i|B)$ 称之为**后验概率**，而且根据公式不难看出，后验概率的大小很受先验概率大小的影响，而实际上在贝叶斯公式的使用中，最有争议的点就是先验概率的选取。

&emsp;&emsp;如何理解这个公式的意义呢？拿医疗诊断来距离，医生为了诊断病人到底是患了疾病 $A_1,A_2,...,A_n$ 中的哪一种，想通过检查某一个指标 $B$ 来判断（比如体温，脉搏等等），换句话说，就是在指标 $B$ 的前提下，判断出哪一种疾病的概率最高，也就是求出 $P(A_i|B)$ ，那我们就先需要先验概率 $P(A_i)$ ，之后再根据往日的资料等等确定 $P(B|A_i)$ ，最后根据贝叶斯公式求出 $P(A_i|B)$ ，我们需要找到最大的 $P(A_i|B)$ ，对应的 $A_i$ 就是出现了指标 $B$ 的病人最有可能患有的疾病。

> 实际上，在医疗检查中，检查的指标往往会有多个，最后综合所有的后验概率来进行判断，颇有实用价值。

## 事件独立性

&emsp;&emsp;有的时候两个事件是不会相互影响，比如说一口袋里有 $a$ 个白球和 $b$ 个黑球，有放回的摸球，那么事件 $B$ 第二次摸到黑球的概率就是 $\frac{a}{a+b}$ ，而且因为是有放回，所以第一次摸到什么颜色的球并不重要，倘若记事件 $A$ 为第一次摸到黑球，那我们不难验证 $P(AB)$ 与 $P(A)P(B)$ 是大小相同的，这很自然，因为第一次摸球根本不会影响到第二次，换句话说，事件 $A$ 与 $B$ 之间有某种“独立性”。

&emsp;&emsp;对此，我们引进该定义：对事件 $A$ 与 $B$ ，如果有：

$$\begin{align\*}
P(AB)=P(A)P(B)
\end{align\*}$$

那么称事件 $A$ 与 $B$ 是**统计独立的**，简称独立的（independent）。按照这个定义，必然事件 $\Omega$ 和不可能事件 $\phi$ 与任何事件独立，而且不难验证，我们有下面的推论：
- 若事件 $A$ 与 $B$ 独立，且 $P(B)>0$ ，则 $P(A|B)=P(A)$
- 若事件 $A$ 与 $B$ 独立，则事件 $\overline{A}$ 与 $B$ 独立， $A$ 与 $\overline{B}$ 独立， $\overline{A}$ 与 $\overline{B}$ 独立（依据定义即可证明）。

&emsp;&emsp;那我们如何定义多个事件的独立性呢？倘若我们有三个事件 $A,B,C$ ，那么我们需要同时满足下面的四个等式：

$$\begin{align\*}
P(AB)=P(A)P(B)\\
P(AC)=P(A)P(C)\\
P(BC)=P(B)P(C)\\
P(ABC)=P(A)P(B)P(C)\\
\end{align\*}$$

才能称这三个事件相互独立。但是自然就会有这样一个问题：我们知道如果有前三个等式成立，就有了事件 $A,B,C$ 两两独立，那为什么还需要第四个等式呢？也就是两两独立是否等于相互独立？我们来看下面的例子：

&emsp;&emsp;（伯恩斯坦反例）一个均匀的正四面体，其第一面染成红色，第二面染成白色，第三面染成黑色，第四面同时染上红白黑这三种颜色，现在以 $A,B,C$ 分别表示抛掷一次四面体出现红、白、黑颜色朝下的事件，由于在四面体中有两面有红色，因此 $P(A)=\frac{1}{2}$ ，同理 $P(B)=P(C)=\frac{1}{2}$ ，而对于事件 $AB,AC,BC$ ，只能是第四面朝下，所以有 $P(AB)=P(AC)=P(BC)=\frac{1}{4}$ （这显然满足前三个等式），但是对于事件 $ABC$ ，我们有 $P(ABC)=\frac{1}{4}\neq\frac{1}{8}=P(A)P(B)P(C)$ ，也就是说，两两独立并不等于相互独立。

&emsp;&emsp;那为什么不能只需要等式 $P(ABC)=P(A)P(B)P(C)$ 来定义相互独立呢？实际上，这个等式不能推导出前三个等式，也就是说，仅凭第四个等式不能推导出事件两两独立。

&emsp;&emsp;若有一个均匀正八面体，其第1，2，3，4面染上红色，第1，2，3，5面染上白色，第1，6，7，8面染上黑色，现已 $A,B,C$ 分别表示抛掷一次八面体出现红、白、黑颜色朝下的事件，显然就有 $P(A)=P(B)=P(C)=\frac{1}{2}$ ，而因为只有第一面同时有三种颜色，所以 $P(ABC)=\frac{1}{8}=P(A)P(B)P(C)$ ，但是 $P(AB)=\frac{3}{8}\neq\frac{1}{4}=P(A)P(B)$ ，这样就说明了，为了定义三个事件的独立性，这四个等式必不可少。

&emsp;&emsp;现在我们可以定义 $n$ 个事件的独立性。

&emsp;&emsp;对 $n$ 个事件 $A_1,A_2,...,A_n$ ，若对于所有可能的组合 $1\leqslant i< j< k<...\leqslant n$ ，有下面这些等式成立：

$$\begin{align\*}
P(A_iA_j)=P(A_i)P(A_j)\\
P(A_iA_jA_k)=P(A_i)P(A_j)P(A_k)\\
...\\
P(A_1A_2...A_n)=P(A_1)P(A_2)...P(A_n)\\
\end{align\*}$$

则称事件 $A_1,A_2,...,A_n$ **相互独立**。

&emsp;&emsp;显然，若 $n$ 个事件相互独立，则他们中的任何 $m(2\leqslant m<n)$ 个事件也相互独立，并且也有与两个事件相互独立类似的推论。另外，对于无穷多个事件，若其中任意有限多个事件都相互独立，则称这无穷多个事件是相互独立的。

&emsp;&emsp;有了事件独立性的概念，我们就可以定义实验的独立性。直观上，实验 $E_1$ 与实验 $E_2$ 要能说是独立的，应是指实验 $E_1$ 的结果的发生与实验 $E_2$ 的结果的发生是独立的，所以自然想到通过各实验的事件间的独立性来定义实验的独立性。为了做到这点，首先要构造一个能够描述这些实验的公共的样本空间。

&emsp;&emsp;设实验 $E_1$ 的样本空间是 $\Omega_1=\\{\omega^{(1)}\\}$ ，实验 $E_2$ 的样本空间是 $\Omega_2=\\{\omega^{(2)}\\}$ ...实验 $E_n$ 的样本空间是 $\Omega_n=\\{\omega^{(n)}\\}$ ，为了描述这 $n$ 次实验，应构造复合实验 $E$ ，它表示依次进行实验 $E_1,E_2,...,E_n$ ，其样本点为

$$\begin{align\*}
\omega=(\omega^{(1)},\omega^{(2)},...,\omega^{(n)})
\end{align\*}$$

这个样本空间为 $\Omega_1,\Omega_2,...,\Omega_n$ 的乘积空间，记作

$$\begin{align\*}
\Omega=\Omega_1\times\Omega_2\times...\times\Omega_n
\end{align\*}$$

&emsp;&emsp;举个通俗些的例子，比如说第一个实验是抛掷一枚硬币，第二个实验是从装有红黑白三球的袋子里摸出一个球，那么这两个实验的样本空间的乘积空间里就会有6个样本点：（正，红），（正，白），（正，黑），（反，红），（反，白），（反，黑），这样也许有助于理解。

&emsp;&emsp;接着我们要引进“与第 $k$ 次实验有关的事件”的概念，这种事件的发生仅与第 $k$ 次实验的结果有关，为了判断乘积空间中某一样本点是否属于这个事件，只需要查看该样本点的第 $k$ 个分量，在刚才的例子中，摸出红球显然就是一个与第2次实验有关的事件。值得指出的是，必然事件与不可能事件可以认为与所有的事件都有关。现在如果以 $\mathscr{F}_k$ 表示与第 $k$ 次实验有关的事件全体，则可以以下列方式来定义实验的独立性。

&emsp;&emsp;若对任意的

$$\begin{align\*}
A^{(1)}\in \mathscr{F}_1,A^{(2)}\in \mathscr{F}_2,...,A^{(n)}\in \mathscr{F}_n
\end{align\*}$$

均有下面的等式成立

$$\begin{align\*}
P(A^{(1)}A^{(2)}...A^{(n)})=P(A^{(1)})P(A^{(2)})...P(A^{(n)})
\end{align\*}$$

则称实验 $E_1,E_2,...,E_n$ 是**相互独立的**。

（说实在话，我写到这里时实在是不知道相互独立实验这一概念到底重要不重要）

## 伯努利实验与直线上的随机游动

&emsp;&emsp;在很多情况下，我们只会关心实验中某一事件 $A$ 是否会发生，就像产品抽样抽到的是合格品还是废品，抛硬币是正面还是反面，股票是涨还是跌，在这类问题中我们可以把事件域取为 $\mathscr{F}_k=\\{\phi,A,\overline{A},\Omega\\}$ ，并称出现 $A$ 为“成功”，出现 $\overline{A}$ 为“失败”，这种只有可能有两个结果的实验称为**伯努利实验**。

&emsp;&emsp;在伯努利实验中，首先是要给出下面的概率：

$$\begin{align\*}
P(A)=p,P(\overline{A})=q
\end{align\*}$$

显然 $p\geqslant0$ ， $q\geqslant0$ ，且 $p+q=1$ 

&emsp;&emsp;现在考虑重复进行 $n$ 次独立的伯努利实验，这里的“重复”是指在每次实验中事件 $A$ 出现的概率都保持不变，这种实验称为 $n$ 重伯努利实验，记作 $E^n$ 。


















